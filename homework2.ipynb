{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preranaramteke/board-infinity-homework/blob/master/homework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50-nTHje5qa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2qhsMuR6v4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#age if else"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8hSVmfg5xNL",
        "colab_type": "code",
        "outputId": "1808e957-4c59-46b9-ad91-5d528ba1bedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"what is your age\\n?\")\n",
        "age=int(input())\n",
        "if age>19:\n",
        "  print('adult')\n",
        "else:\n",
        "  print(\"teenager\")  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what is your age\n",
            "?\n",
            "17\n",
            "teenager\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udyDUlrN7Mct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loan approving system"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B48nEIR7YmP",
        "colab_type": "code",
        "outputId": "7997cd3c-725a-4650-d855-54fcec463e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print('what is your monthly income?\\n')\n",
        "income=int(input())\n",
        "print('Do you have any ongoing loan?\\n')\n",
        "ongoing_loan=str(input())\n",
        "if income>100000 and ongoing_loan=='no':\n",
        "  print('you are eligible')\n",
        "  if income>100000 and ongoing_loan=='yes':\n",
        "    print('you are not eligibile')\n",
        "else:\n",
        "  print('you are not eligibile')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what is your monthly income?\n",
            "\n",
            "10000000\n",
            "Do you have any ongoing loan?\n",
            "\n",
            "no\n",
            "you are eligible\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U6nU62E853w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#You are female"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3O96PH_-EW6",
        "colab_type": "code",
        "outputId": "946f38fe-c6ac-4a2c-b2b6-48519ca790cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('are you a female?\\n')\n",
        "female=str(input())\n",
        "if female=='yes':\n",
        "  print('you are a girl')\n",
        "else:\n",
        "  print('you are a boy')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "are you a female?\n",
            "\n",
            "no\n",
            "you are a boy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0zNb419K13r",
        "colab_type": "code",
        "outputId": "b87d8ed6-cfc7-4e99-ae2c-3f986c60fe9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "#list of random numbers\n",
        "list=[1,2,3,4,5,6,7,8,9]\n",
        "for x in list:\n",
        "  print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT1f2--HLjYH",
        "colab_type": "code",
        "outputId": "970b1886-1c03-4192-ff5a-ef8eb8a3f280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#even numbers\n",
        "list=[1,2,3,4,5,6,7,8,9,10]\n",
        "for x in list:\n",
        "  if x%2==0:\n",
        "    print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYaTit9OL1H7",
        "colab_type": "code",
        "outputId": "884014d5-79a7-412d-c61a-c048a4025c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#dictionary of myself\n",
        "myself={'Name':'Prerana','Age':21,'gender':'female','fav.food':'paneer'}\n",
        "for x in myself:\n",
        "  print(myself.keys())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['Name', 'Age', 'gender', 'fav.food'])\n",
            "dict_keys(['Name', 'Age', 'gender', 'fav.food'])\n",
            "dict_keys(['Name', 'Age', 'gender', 'fav.food'])\n",
            "dict_keys(['Name', 'Age', 'gender', 'fav.food'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHmOSFdsMLJb",
        "colab_type": "code",
        "outputId": "d3b97f85-4d2b-4f6a-a824-fabc29b95c12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "#dictionary values\n",
        "for x in myself:\n",
        "  print(myself.values())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_values(['Pradnya Karve', 24, 'female', 'Dosa'])\n",
            "dict_values(['Pradnya Karve', 24, 'female', 'Dosa'])\n",
            "dict_values(['Pradnya Karve', 24, 'female', 'Dosa'])\n",
            "dict_values(['Pradnya Karve', 24, 'female', 'Dosa'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUZnL3EJNFP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#funcion name\n",
        "def person_name(name):\n",
        "  return(name)\n",
        "name=\"prerana\"  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sShS0nXaX-8R",
        "colab_type": "code",
        "outputId": "58812150-64b7-43c8-97b8-22c297c50429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "name"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prerana'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36dIHZrrYHCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dictionary function\n",
        "def dict_func(dict):\n",
        "  return(dict.keys)\n",
        "  dict={'name':\"prerana\",'age':21,'gender':'female'}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoUIDPHtY4i6",
        "colab_type": "code",
        "outputId": "20351b5b-5d99-47e9-ded1-ef8615aaddfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['name', 'age', 'gender'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCEe6Ur2Y5SN",
        "colab_type": "code",
        "outputId": "9b089369-6878-48b8-a20d-e2afa1cf6a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#random numbrs function\n",
        "#def randon_numbers(num):\n",
        "    #for a in random_numbers:\n",
        "     # if a%2!=0:\n",
        "      #   print(a)\n",
        "#num=[1,2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "#random_numbers(num)\n",
        "\n",
        "      \n",
        "\n",
        "#type conversion\n",
        "\n",
        "#Str\n",
        "value='24'\n",
        "\n",
        "#float\n",
        "tenth_marks=89.27\n",
        "\n",
        "#boolean\n",
        "female=True\n",
        "\n",
        "#integer\n",
        "age=24\n",
        "\n",
        "#int to float\n",
        "age=float(age)\n",
        "print(type(age))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'float'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txO6-dCObDf9",
        "colab_type": "code",
        "outputId": "c33ff169-ec23-4eac-8d56-dbfb45619d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#string to integer\n",
        "value=int(value)\n",
        "print(type(value))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'int'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPM0WqKYbIrK",
        "colab_type": "code",
        "outputId": "532ff132-85ac-4d89-f879-00e8be529a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#intger to string\n",
        "age=str(age)\n",
        "print(type(age))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ksqfgyzDL75",
        "colab_type": "code",
        "outputId": "05eda99b-928d-45bd-c46f-d8ac27789c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#boolean to str\n",
        "female=str(female)\n",
        "print(type(female))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pijeH-NWCFw_",
        "colab_type": "text"
      },
      "source": [
        "# importing sklearn Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h9EX6LuDSxE",
        "colab_type": "code",
        "outputId": "5e720a62-0cca-4ca8-9ed4-d5e9ff4c347c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#sklearn=class\n",
        "from sklearn import datasets\n",
        "type(datasets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "module"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGD13rcoEPbs",
        "colab_type": "code",
        "outputId": "e64e1d02-e2a5-43b3-c826-3d5487f52b46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBAi2g268b9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI2HBqGZ8k1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDOhP1BH8sI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIa4Yao1y9EF",
        "colab_type": "text"
      },
      "source": [
        "## importing random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCHFfK7OzDDF",
        "colab_type": "code",
        "outputId": "1c6cf533-b2c8-44d1-dff5-f0eb732009aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "help(RandomForestClassifier)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
            "\n",
            "class RandomForestClassifier(ForestClassifier)\n",
            " |  A random forest classifier.\n",
            " |  \n",
            " |  A random forest is a meta estimator that fits a number of decision tree\n",
            " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
            " |  improve the predictive accuracy and control over-fitting.\n",
            " |  The sub-sample size is always the same as the original\n",
            " |  input sample size but the samples are drawn with replacement if\n",
            " |  `bootstrap=True` (default).\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <forest>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  n_estimators : integer, optional (default=100)\n",
            " |      The number of trees in the forest.\n",
            " |  \n",
            " |      .. versionchanged:: 0.22\n",
            " |         The default value of ``n_estimators`` changed from 10 to 100\n",
            " |         in 0.22.\n",
            " |  \n",
            " |  criterion : string, optional (default=\"gini\")\n",
            " |      The function to measure the quality of a split. Supported criteria are\n",
            " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
            " |      Note: this parameter is tree-specific.\n",
            " |  \n",
            " |  max_depth : integer or None, optional (default=None)\n",
            " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
            " |      all leaves are pure or until all leaves contain less than\n",
            " |      min_samples_split samples.\n",
            " |  \n",
            " |  min_samples_split : int, float, optional (default=2)\n",
            " |      The minimum number of samples required to split an internal node:\n",
            " |  \n",
            " |      - If int, then consider `min_samples_split` as the minimum number.\n",
            " |      - If float, then `min_samples_split` is a fraction and\n",
            " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
            " |        number of samples for each split.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_samples_leaf : int, float, optional (default=1)\n",
            " |      The minimum number of samples required to be at a leaf node.\n",
            " |      A split point at any depth will only be considered if it leaves at\n",
            " |      least ``min_samples_leaf`` training samples in each of the left and\n",
            " |      right branches.  This may have the effect of smoothing the model,\n",
            " |      especially in regression.\n",
            " |  \n",
            " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
            " |      - If float, then `min_samples_leaf` is a fraction and\n",
            " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            " |        number of samples for each node.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
            " |      The minimum weighted fraction of the sum total of weights (of all\n",
            " |      the input samples) required to be at a leaf node. Samples have\n",
            " |      equal weight when sample_weight is not provided.\n",
            " |  \n",
            " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
            " |      The number of features to consider when looking for the best split:\n",
            " |  \n",
            " |      - If int, then consider `max_features` features at each split.\n",
            " |      - If float, then `max_features` is a fraction and\n",
            " |        `int(max_features * n_features)` features are considered at each\n",
            " |        split.\n",
            " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
            " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
            " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
            " |      - If None, then `max_features=n_features`.\n",
            " |  \n",
            " |      Note: the search for a split does not stop until at least one\n",
            " |      valid partition of the node samples is found, even if it requires to\n",
            " |      effectively inspect more than ``max_features`` features.\n",
            " |  \n",
            " |  max_leaf_nodes : int or None, optional (default=None)\n",
            " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            " |      Best nodes are defined as relative reduction in impurity.\n",
            " |      If None then unlimited number of leaf nodes.\n",
            " |  \n",
            " |  min_impurity_decrease : float, optional (default=0.)\n",
            " |      A node will be split if this split induces a decrease of the impurity\n",
            " |      greater than or equal to this value.\n",
            " |  \n",
            " |      The weighted impurity decrease equation is the following::\n",
            " |  \n",
            " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            " |                              - N_t_L / N_t * left_impurity)\n",
            " |  \n",
            " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
            " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
            " |  \n",
            " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            " |      if ``sample_weight`` is passed.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |  \n",
            " |  min_impurity_split : float, (default=1e-7)\n",
            " |      Threshold for early stopping in tree growth. A node will split\n",
            " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
            " |  \n",
            " |      .. deprecated:: 0.19\n",
            " |         ``min_impurity_split`` has been deprecated in favor of\n",
            " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
            " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
            " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
            " |  \n",
            " |  \n",
            " |  bootstrap : boolean, optional (default=True)\n",
            " |      Whether bootstrap samples are used when building trees. If False, the\n",
            " |      whole datset is used to build each tree.\n",
            " |  \n",
            " |  oob_score : bool (default=False)\n",
            " |      Whether to use out-of-bag samples to estimate\n",
            " |      the generalization accuracy.\n",
            " |  \n",
            " |  n_jobs : int or None, optional (default=None)\n",
            " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
            " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
            " |      <n_jobs>` for more details.\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, optional (default=None)\n",
            " |      Controls both the randomness of the bootstrapping of the samples used\n",
            " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
            " |      features to consider when looking for the best split at each node\n",
            " |      (if ``max_features < n_features``).\n",
            " |      See :term:`Glossary <random_state>` for details.\n",
            " |  \n",
            " |  verbose : int, optional (default=0)\n",
            " |      Controls the verbosity when fitting and predicting.\n",
            " |  \n",
            " |  warm_start : bool, optional (default=False)\n",
            " |      When set to ``True``, reuse the solution of the previous call to fit\n",
            " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
            " |      new forest. See :term:`the Glossary <warm_start>`.\n",
            " |  \n",
            " |  class_weight : dict, list of dicts, \"balanced\", \"balanced_subsample\" or     None, optional (default=None)\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If not given, all classes are supposed to have weight one. For\n",
            " |      multi-output problems, a list of dicts can be provided in the same\n",
            " |      order as the columns of y.\n",
            " |  \n",
            " |      Note that for multioutput (including multilabel) weights should be\n",
            " |      defined for each class of every column in its own dict. For example,\n",
            " |      for four-class multilabel classification weights should be\n",
            " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            " |  \n",
            " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
            " |      weights are computed based on the bootstrap sample for every tree\n",
            " |      grown.\n",
            " |  \n",
            " |      For multi-output, the weights of each column of y will be multiplied.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |  ccp_alpha : non-negative float, optional (default=0.0)\n",
            " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            " |      subtree with the largest cost complexity that is smaller than\n",
            " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  max_samples : int or float, default=None\n",
            " |      If bootstrap is True, the number of samples to draw from X\n",
            " |      to train each base estimator.\n",
            " |  \n",
            " |      - If None (default), then draw `X.shape[0]` samples.\n",
            " |      - If int, then draw `max_samples` samples.\n",
            " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
            " |        `max_samples` should be in the interval `(0, 1)`.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  base_estimator_ : DecisionTreeClassifier\n",
            " |      The child estimator template used to create the collection of fitted\n",
            " |      sub-estimators.\n",
            " |  \n",
            " |  estimators_ : list of DecisionTreeClassifier\n",
            " |      The collection of fitted sub-estimators.\n",
            " |  \n",
            " |  classes_ : array of shape (n_classes,) or a list of such arrays\n",
            " |      The classes labels (single output problem), or a list of arrays of\n",
            " |      class labels (multi-output problem).\n",
            " |  \n",
            " |  n_classes_ : int or list\n",
            " |      The number of classes (single output problem), or a list containing the\n",
            " |      number of classes for each output (multi-output problem).\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      The number of features when ``fit`` is performed.\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      The number of outputs when ``fit`` is performed.\n",
            " |  \n",
            " |  feature_importances_ : ndarray of shape (n_features,)\n",
            " |      The feature importances (the higher, the more important the feature).\n",
            " |  \n",
            " |  oob_score_ : float\n",
            " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
            " |      This attribute exists only when ``oob_score`` is True.\n",
            " |  \n",
            " |  oob_decision_function_ : array of shape (n_samples, n_classes)\n",
            " |      Decision function computed with out-of-bag estimate on the training\n",
            " |      set. If n_estimators is small it might be possible that a data point\n",
            " |      was never left out during the bootstrap. In this case,\n",
            " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
            " |      only when ``oob_score`` is True.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
            " |  >>> from sklearn.datasets import make_classification\n",
            " |  \n",
            " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
            " |  ...                            n_informative=2, n_redundant=0,\n",
            " |  ...                            random_state=0, shuffle=False)\n",
            " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
            " |  >>> clf.fit(X, y)\n",
            " |  RandomForestClassifier(max_depth=2, random_state=0)\n",
            " |  >>> print(clf.feature_importances_)\n",
            " |  [0.14205973 0.76664038 0.0282433  0.06305659]\n",
            " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
            " |  [1]\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The default values for the parameters controlling the size of the trees\n",
            " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            " |  unpruned trees which can potentially be very large on some data sets. To\n",
            " |  reduce memory consumption, the complexity and size of the trees should be\n",
            " |  controlled by setting those parameter values.\n",
            " |  \n",
            " |  The features are always randomly permuted at each split. Therefore,\n",
            " |  the best found split may vary, even with the same training data,\n",
            " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
            " |  of the criterion is identical for several splits enumerated during the\n",
            " |  search of the best split. To obtain a deterministic behaviour during\n",
            " |  fitting, ``random_state`` has to be fixed.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  \n",
            " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      RandomForestClassifier\n",
            " |      ForestClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      BaseForest\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.ensemble._base.BaseEnsemble\n",
            " |      sklearn.base.MetaEstimatorMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from ForestClassifier:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict class for X.\n",
            " |      \n",
            " |      The predicted class of an input sample is a vote by the trees in\n",
            " |      the forest, weighted by their probability estimates. That is,\n",
            " |      the predicted class is the one with highest mean probability\n",
            " |      estimate across the trees.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The predicted classes.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict class log-probabilities for X.\n",
            " |      \n",
            " |      The predicted class log-probabilities of an input sample is computed as\n",
            " |      the log of the mean predicted class probabilities of the trees in the\n",
            " |      forest.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      p : array of shape (n_samples, n_classes), or a list of n_outputs\n",
            " |          such arrays if n_outputs > 1.\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Predict class probabilities for X.\n",
            " |      \n",
            " |      The predicted class probabilities of an input sample are computed as\n",
            " |      the mean predicted class probabilities of the trees in the forest.\n",
            " |      The class probability of a single tree is the fraction of samples of\n",
            " |      the same class in a leaf.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      p : array of shape (n_samples, n_classes), or a list of n_outputs\n",
            " |          such arrays if n_outputs > 1.\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseForest:\n",
            " |  \n",
            " |  apply(self, X)\n",
            " |      Apply trees in the forest to X, return leaf indices.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
            " |          For each datapoint x in X and for each tree in the forest,\n",
            " |          return the index of the leaf x ends up in.\n",
            " |  \n",
            " |  decision_path(self, X)\n",
            " |      Return the decision path in the forest.\n",
            " |      \n",
            " |      .. versionadded:: 0.18\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
            " |          Return a node indicator matrix where non zero elements\n",
            " |          indicates that the samples goes through the nodes.\n",
            " |      \n",
            " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
            " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
            " |          gives the indicator value for the i-th estimator.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Build a forest of trees from the training set (X, y).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like or sparse matrix of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, its dtype will be converted\n",
            " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels in classification, real numbers in\n",
            " |          regression).\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. In the case of\n",
            " |          classification, splits are also ignored if they would result in any\n",
            " |          single class carrying a negative weight in either child node.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseForest:\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      Return the feature importances (the higher, the more important the\n",
            " |         feature).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : array, shape = [n_features]\n",
            " |          The values of this array sum to 1, unless all trees are single node\n",
            " |          trees consisting of only the root node, in which case it will be an\n",
            " |          array of zeros.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
            " |  \n",
            " |  __getitem__(self, index)\n",
            " |      Return the index'th estimator in the ensemble.\n",
            " |  \n",
            " |  __iter__(self)\n",
            " |      Return iterator over estimators in the ensemble.\n",
            " |  \n",
            " |  __len__(self)\n",
            " |      Return the number of estimators in the ensemble.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Scr_N43Byz-",
        "colab_type": "text"
      },
      "source": [
        "# importing Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC3yUI6r8_QA",
        "colab_type": "code",
        "outputId": "c6414ce1-be49-4823-8fd0-b96bc0ae75de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#decisionTree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "help(DecisionTreeClassifier)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
            "\n",
            "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
            " |  A decision tree classifier.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <tree>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
            " |      The function to measure the quality of a split. Supported criteria are\n",
            " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
            " |  \n",
            " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
            " |      The strategy used to choose the split at each node. Supported\n",
            " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
            " |      the best random split.\n",
            " |  \n",
            " |  max_depth : int, default=None\n",
            " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
            " |      all leaves are pure or until all leaves contain less than\n",
            " |      min_samples_split samples.\n",
            " |  \n",
            " |  min_samples_split : int or float, default=2\n",
            " |      The minimum number of samples required to split an internal node:\n",
            " |  \n",
            " |      - If int, then consider `min_samples_split` as the minimum number.\n",
            " |      - If float, then `min_samples_split` is a fraction and\n",
            " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
            " |        number of samples for each split.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_samples_leaf : int or float, default=1\n",
            " |      The minimum number of samples required to be at a leaf node.\n",
            " |      A split point at any depth will only be considered if it leaves at\n",
            " |      least ``min_samples_leaf`` training samples in each of the left and\n",
            " |      right branches.  This may have the effect of smoothing the model,\n",
            " |      especially in regression.\n",
            " |  \n",
            " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
            " |      - If float, then `min_samples_leaf` is a fraction and\n",
            " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            " |        number of samples for each node.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_weight_fraction_leaf : float, default=0.0\n",
            " |      The minimum weighted fraction of the sum total of weights (of all\n",
            " |      the input samples) required to be at a leaf node. Samples have\n",
            " |      equal weight when sample_weight is not provided.\n",
            " |  \n",
            " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
            " |      The number of features to consider when looking for the best split:\n",
            " |  \n",
            " |          - If int, then consider `max_features` features at each split.\n",
            " |          - If float, then `max_features` is a fraction and\n",
            " |            `int(max_features * n_features)` features are considered at each\n",
            " |            split.\n",
            " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
            " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
            " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
            " |          - If None, then `max_features=n_features`.\n",
            " |  \n",
            " |      Note: the search for a split does not stop until at least one\n",
            " |      valid partition of the node samples is found, even if it requires to\n",
            " |      effectively inspect more than ``max_features`` features.\n",
            " |  \n",
            " |  random_state : int or RandomState, default=None\n",
            " |      If int, random_state is the seed used by the random number generator;\n",
            " |      If RandomState instance, random_state is the random number generator;\n",
            " |      If None, the random number generator is the RandomState instance used\n",
            " |      by `np.random`.\n",
            " |  \n",
            " |  max_leaf_nodes : int, default=None\n",
            " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
            " |      Best nodes are defined as relative reduction in impurity.\n",
            " |      If None then unlimited number of leaf nodes.\n",
            " |  \n",
            " |  min_impurity_decrease : float, default=0.0\n",
            " |      A node will be split if this split induces a decrease of the impurity\n",
            " |      greater than or equal to this value.\n",
            " |  \n",
            " |      The weighted impurity decrease equation is the following::\n",
            " |  \n",
            " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            " |                              - N_t_L / N_t * left_impurity)\n",
            " |  \n",
            " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
            " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
            " |  \n",
            " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            " |      if ``sample_weight`` is passed.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |  \n",
            " |  min_impurity_split : float, default=1e-7\n",
            " |      Threshold for early stopping in tree growth. A node will split\n",
            " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
            " |  \n",
            " |      .. deprecated:: 0.19\n",
            " |         ``min_impurity_split`` has been deprecated in favor of\n",
            " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
            " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
            " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
            " |  \n",
            " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If None, all classes are supposed to have weight one. For\n",
            " |      multi-output problems, a list of dicts can be provided in the same\n",
            " |      order as the columns of y.\n",
            " |  \n",
            " |      Note that for multioutput (including multilabel) weights should be\n",
            " |      defined for each class of every column in its own dict. For example,\n",
            " |      for four-class multilabel classification weights should be\n",
            " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            " |  \n",
            " |      For multi-output, the weights of each column of y will be multiplied.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |  presort : deprecated, default='deprecated'\n",
            " |      This parameter is deprecated and will be removed in v0.24.\n",
            " |  \n",
            " |      .. deprecated:: 0.22\n",
            " |  \n",
            " |  ccp_alpha : non-negative float, default=0.0\n",
            " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            " |      subtree with the largest cost complexity that is smaller than\n",
            " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
            " |      The classes labels (single output problem),\n",
            " |      or a list of arrays of class labels (multi-output problem).\n",
            " |  \n",
            " |  feature_importances_ : ndarray of shape (n_features,)\n",
            " |      The feature importances. The higher, the more important the\n",
            " |      feature. The importance of a feature is computed as the (normalized)\n",
            " |      total reduction of the criterion brought by that feature.  It is also\n",
            " |      known as the Gini importance [4]_.\n",
            " |  \n",
            " |  max_features_ : int\n",
            " |      The inferred value of max_features.\n",
            " |  \n",
            " |  n_classes_ : int or list of int\n",
            " |      The number of classes (for single output problems),\n",
            " |      or a list containing the number of classes for each\n",
            " |      output (for multi-output problems).\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      The number of features when ``fit`` is performed.\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      The number of outputs when ``fit`` is performed.\n",
            " |  \n",
            " |  tree_ : Tree\n",
            " |      The underlying Tree object. Please refer to\n",
            " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
            " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
            " |      for basic usage of these attributes.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  DecisionTreeRegressor : A decision tree regressor.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The default values for the parameters controlling the size of the trees\n",
            " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            " |  unpruned trees which can potentially be very large on some data sets. To\n",
            " |  reduce memory consumption, the complexity and size of the trees should be\n",
            " |  controlled by setting those parameter values.\n",
            " |  \n",
            " |  The features are always randomly permuted at each split. Therefore,\n",
            " |  the best found split may vary, even with the same training data and\n",
            " |  ``max_features=n_features``, if the improvement of the criterion is\n",
            " |  identical for several splits enumerated during the search of the best\n",
            " |  split. To obtain a deterministic behaviour during fitting,\n",
            " |  ``random_state`` has to be fixed.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  \n",
            " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
            " |  \n",
            " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
            " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
            " |  \n",
            " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
            " |         Learning\", Springer, 2009.\n",
            " |  \n",
            " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
            " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.datasets import load_iris\n",
            " |  >>> from sklearn.model_selection import cross_val_score\n",
            " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
            " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
            " |  >>> iris = load_iris()\n",
            " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
            " |  ...                             # doctest: +SKIP\n",
            " |  ...\n",
            " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
            " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DecisionTreeClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      BaseDecisionTree\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
            " |      Build a decision tree classifier from the training set (X, y).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels) as integers or strings.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. Splits are also\n",
            " |          ignored if they would result in any single class carrying a\n",
            " |          negative weight in either child node.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      X_idx_sorted : array-like of shape (n_samples, n_features),                 default=None\n",
            " |          The indexes of the sorted training input samples. If many tree\n",
            " |          are grown on the same dataset, this allows the ordering to be\n",
            " |          cached between trees. If None, the data will be sorted here.\n",
            " |          Don't use this parameter unless you know what to do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : DecisionTreeClassifier\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict class log-probabilities of the input samples X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
            " |          The class log-probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X, check_input=True)\n",
            " |      Predict class probabilities of the input samples X.\n",
            " |      \n",
            " |      The predicted class probability is the fraction of samples of the same\n",
            " |      class in a leaf.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseDecisionTree:\n",
            " |  \n",
            " |  apply(self, X, check_input=True)\n",
            " |      Return the index of the leaf that each sample is predicted as.\n",
            " |      \n",
            " |      .. versionadded:: 0.17\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : array-like of shape (n_samples,)\n",
            " |          For each datapoint x in X, return the index of the leaf x\n",
            " |          ends up in. Leaves are numbered within\n",
            " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
            " |          numbering.\n",
            " |  \n",
            " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
            " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
            " |      \n",
            " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
            " |      process.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels) as integers or strings.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. Splits are also\n",
            " |          ignored if they would result in any single class carrying a\n",
            " |          negative weight in either child node.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ccp_path : Bunch\n",
            " |          Dictionary-like object, with attributes:\n",
            " |      \n",
            " |          ccp_alphas : ndarray\n",
            " |              Effective alphas of subtree during pruning.\n",
            " |      \n",
            " |          impurities : ndarray\n",
            " |              Sum of the impurities of the subtree leaves for the\n",
            " |              corresponding alpha value in ``ccp_alphas``.\n",
            " |  \n",
            " |  decision_path(self, X, check_input=True)\n",
            " |      Return the decision path in the tree.\n",
            " |      \n",
            " |      .. versionadded:: 0.18\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
            " |          Return a node indicator CSR matrix where non zero elements\n",
            " |          indicates that the samples goes through the nodes.\n",
            " |  \n",
            " |  get_depth(self)\n",
            " |      Return the depth of the decision tree.\n",
            " |      \n",
            " |      The depth of a tree is the maximum distance between the root\n",
            " |      and any leaf.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self.tree_.max_depth : int\n",
            " |          The maximum depth of the tree.\n",
            " |  \n",
            " |  get_n_leaves(self)\n",
            " |      Return the number of leaves of the decision tree.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self.tree_.n_leaves : int\n",
            " |          Number of leaves.\n",
            " |  \n",
            " |  predict(self, X, check_input=True)\n",
            " |      Predict class or regression value for X.\n",
            " |      \n",
            " |      For a classification model, the predicted class for each sample in X is\n",
            " |      returned. For a regression model, the predicted value based on X is\n",
            " |      returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, it will be converted to\n",
            " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
            " |          to a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      check_input : bool, default=True\n",
            " |          Allow to bypass several input checking.\n",
            " |          Don't use this parameter unless you know what you do.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The predicted classes, or the predict values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseDecisionTree:\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      Return the feature importances.\n",
            " |      \n",
            " |      The importance of a feature is computed as the (normalized) total\n",
            " |      reduction of the criterion brought by that feature.\n",
            " |      It is also known as the Gini importance.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : ndarray of shape (n_features,)\n",
            " |          Normalized total reduction of criteria by feature\n",
            " |          (Gini importance).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8jSDQs7-x77",
        "colab_type": "text"
      },
      "source": [
        "**# importing Naive bayes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSoAD7sn-ZHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ZOFJmR-hRj",
        "colab_type": "code",
        "outputId": "48d0da45-5cb9-41b4-f665-97992f579735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "help(GaussianNB)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class GaussianNB in module sklearn.naive_bayes:\n",
            "\n",
            "class GaussianNB(_BaseNB)\n",
            " |  Gaussian Naive Bayes (GaussianNB)\n",
            " |  \n",
            " |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
            " |  For details on algorithm used to update feature means and variance online,\n",
            " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
            " |  \n",
            " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  priors : array-like, shape (n_classes,)\n",
            " |      Prior probabilities of the classes. If specified the priors are not\n",
            " |      adjusted according to the data.\n",
            " |  \n",
            " |  var_smoothing : float, optional (default=1e-9)\n",
            " |      Portion of the largest variance of all features that is added to\n",
            " |      variances for calculation stability.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  class_count_ : array, shape (n_classes,)\n",
            " |      number of training samples observed in each class.\n",
            " |  \n",
            " |  class_prior_ : array, shape (n_classes,)\n",
            " |      probability of each class.\n",
            " |  \n",
            " |  classes_ : array, shape (n_classes,)\n",
            " |      class labels known to the classifier\n",
            " |  \n",
            " |  epsilon_ : float\n",
            " |      absolute additive value to variances\n",
            " |  \n",
            " |  sigma_ : array, shape (n_classes, n_features)\n",
            " |      variance of each feature per class\n",
            " |  \n",
            " |  theta_ : array, shape (n_classes, n_features)\n",
            " |      mean of each feature per class\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
            " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
            " |  >>> from sklearn.naive_bayes import GaussianNB\n",
            " |  >>> clf = GaussianNB()\n",
            " |  >>> clf.fit(X, Y)\n",
            " |  GaussianNB()\n",
            " |  >>> print(clf.predict([[-0.8, -1]]))\n",
            " |  [1]\n",
            " |  >>> clf_pf = GaussianNB()\n",
            " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
            " |  GaussianNB()\n",
            " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
            " |  [1]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      GaussianNB\n",
            " |      _BaseNB\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, priors=None, var_smoothing=1e-09)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit Gaussian Naive Bayes according to X, y\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like, shape (n_samples, n_features)\n",
            " |          Training vectors, where n_samples is the number of samples\n",
            " |          and n_features is the number of features.\n",
            " |      \n",
            " |      y : array-like, shape (n_samples,)\n",
            " |          Target values.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |      \n",
            " |          .. versionadded:: 0.17\n",
            " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
            " |      Incremental fit on a batch of samples.\n",
            " |      \n",
            " |      This method is expected to be called several times consecutively\n",
            " |      on different chunks of a dataset so as to implement out-of-core\n",
            " |      or online learning.\n",
            " |      \n",
            " |      This is especially useful when the whole dataset is too big to fit in\n",
            " |      memory at once.\n",
            " |      \n",
            " |      This method has some performance and numerical stability overhead,\n",
            " |      hence it is better to call partial_fit on chunks of data that are\n",
            " |      as large as possible (as long as fitting in the memory budget) to\n",
            " |      hide the overhead.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like, shape (n_samples, n_features)\n",
            " |          Training vectors, where n_samples is the number of samples and\n",
            " |          n_features is the number of features.\n",
            " |      \n",
            " |      y : array-like, shape (n_samples,)\n",
            " |          Target values.\n",
            " |      \n",
            " |      classes : array-like, shape (n_classes,), optional (default=None)\n",
            " |          List of all the classes that can possibly appear in the y vector.\n",
            " |      \n",
            " |          Must be provided at the first call to partial_fit, can be omitted\n",
            " |          in subsequent calls.\n",
            " |      \n",
            " |      sample_weight : array-like, shape (n_samples,), optional (default=None)\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |      \n",
            " |          .. versionadded:: 0.17\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _BaseNB:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Perform classification on an array of test vectors X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : ndarray of shape (n_samples,)\n",
            " |          Predicted target values for X\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Return log-probability estimates for the test vector X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the log-probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Return probability estimates for the test vector X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of self.predict(X) wrt. y.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DhXHGUD-nkE",
        "colab_type": "code",
        "outputId": "37a4dee5-2afb-4f16-bcfd-5e3c16f68efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Linear regression\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "help(LinearRegression)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class LinearRegression in module sklearn.linear_model._base:\n",
            "\n",
            "class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
            " |  Ordinary least squares Linear Regression.\n",
            " |  \n",
            " |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
            " |  to minimize the residual sum of squares between the observed targets in\n",
            " |  the dataset, and the targets predicted by the linear approximation.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  fit_intercept : bool, optional, default True\n",
            " |      Whether to calculate the intercept for this model. If set\n",
            " |      to False, no intercept will be used in calculations\n",
            " |      (i.e. data is expected to be centered).\n",
            " |  \n",
            " |  normalize : bool, optional, default False\n",
            " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
            " |      If True, the regressors X will be normalized before regression by\n",
            " |      subtracting the mean and dividing by the l2-norm.\n",
            " |      If you wish to standardize, please use\n",
            " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n",
            " |      an estimator with ``normalize=False``.\n",
            " |  \n",
            " |  copy_X : bool, optional, default True\n",
            " |      If True, X will be copied; else, it may be overwritten.\n",
            " |  \n",
            " |  n_jobs : int or None, optional (default=None)\n",
            " |      The number of jobs to use for the computation. This will only provide\n",
            " |      speedup for n_targets > 1 and sufficient large problems.\n",
            " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            " |      for more details.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
            " |      Estimated coefficients for the linear regression problem.\n",
            " |      If multiple targets are passed during the fit (y 2D), this\n",
            " |      is a 2D array of shape (n_targets, n_features), while if only\n",
            " |      one target is passed, this is a 1D array of length n_features.\n",
            " |  \n",
            " |  rank_ : int\n",
            " |      Rank of matrix `X`. Only available when `X` is dense.\n",
            " |  \n",
            " |  singular_ : array of shape (min(X, y),)\n",
            " |      Singular values of `X`. Only available when `X` is dense.\n",
            " |  \n",
            " |  intercept_ : float or array of shape of (n_targets,)\n",
            " |      Independent term in the linear model. Set to 0.0 if\n",
            " |      `fit_intercept = False`.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  sklearn.linear_model.Ridge : Ridge regression addresses some of the\n",
            " |      problems of Ordinary Least Squares by imposing a penalty on the\n",
            " |      size of the coefficients with l2 regularization.\n",
            " |  sklearn.linear_model.Lasso : The Lasso is a linear model that estimates\n",
            " |      sparse coefficients with l1 regularization.\n",
            " |  sklearn.linear_model.ElasticNet : Elastic-Net is a linear regression\n",
            " |      model trained with both l1 and l2 -norm regularization of the\n",
            " |      coefficients.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  From the implementation point of view, this is just plain Ordinary\n",
            " |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> from sklearn.linear_model import LinearRegression\n",
            " |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
            " |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
            " |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
            " |  >>> reg = LinearRegression().fit(X, y)\n",
            " |  >>> reg.score(X, y)\n",
            " |  1.0\n",
            " |  >>> reg.coef_\n",
            " |  array([1., 2.])\n",
            " |  >>> reg.intercept_\n",
            " |  3.0000...\n",
            " |  >>> reg.predict(np.array([[3, 5]]))\n",
            " |  array([16.])\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LinearRegression\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.RegressorMixin\n",
            " |      LinearModel\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit linear model.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training data\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
            " |          Target values. Will be cast to X's dtype if necessary\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Individual weights for each sample\n",
            " |      \n",
            " |          .. versionadded:: 0.17\n",
            " |             parameter *sample_weight* support to LinearRegression.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : returns an instance of self.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.RegressorMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the coefficient of determination R^2 of the prediction.\n",
            " |      \n",
            " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
            " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
            " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
            " |      The best possible score is 1.0 and it can be negative (because the\n",
            " |      model can be arbitrarily worse). A constant model that always\n",
            " |      predicts the expected value of y, disregarding the input features,\n",
            " |      would get a R^2 score of 0.0.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples. For some estimators this may be a\n",
            " |          precomputed kernel matrix or a list of generic objects instead,\n",
            " |          shape = (n_samples, n_samples_fitted),\n",
            " |          where n_samples_fitted is the number of\n",
            " |          samples used in the fitting for the estimator.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True values for X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          R^2 of self.predict(X) wrt. y.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The R2 score used when calling ``score`` on a regressor will use\n",
            " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
            " |      with :func:`~sklearn.metrics.r2_score`. This will influence the\n",
            " |      ``score`` method of all the multioutput regressors (except for\n",
            " |      :class:`~sklearn.multioutput.MultiOutputRegressor`). To specify the\n",
            " |      default value manually and avoid the warning, please either call\n",
            " |      :func:`~sklearn.metrics.r2_score` directly or make a custom scorer with\n",
            " |      :func:`~sklearn.metrics.make_scorer` (the built-in scorer ``'r2'`` uses\n",
            " |      ``multioutput='uniform_average'``).\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from LinearModel:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict using the linear model.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
            " |          Samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array, shape (n_samples,)\n",
            " |          Returns predicted values.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : mapping of string to any\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as pipelines). The latter have parameters of the form\n",
            " |      ``<component>__<parameter>`` so that it's possible to update each\n",
            " |      component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Estimator instance.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJGknqYVAGQf",
        "colab_type": "code",
        "outputId": "beec43df-9148-42e8-faee-e29d5f7900fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.cluster import k_means\n",
        "help(k_means)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function k_means in module sklearn.cluster._kmeans:\n",
            "\n",
            "k_means(X, n_clusters, sample_weight=None, init='k-means++', precompute_distances='auto', n_init=10, max_iter=300, verbose=False, tol=0.0001, random_state=None, copy_x=True, n_jobs=None, algorithm='auto', return_n_iter=False)\n",
            "    K-means clustering algorithm.\n",
            "    \n",
            "    Read more in the :ref:`User Guide <k_means>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    X : array-like or sparse matrix, shape (n_samples, n_features)\n",
            "        The observations to cluster. It must be noted that the data\n",
            "        will be converted to C ordering, which will cause a memory copy\n",
            "        if the given data is not C-contiguous.\n",
            "    \n",
            "    n_clusters : int\n",
            "        The number of clusters to form as well as the number of\n",
            "        centroids to generate.\n",
            "    \n",
            "    sample_weight : array-like, shape (n_samples,), optional\n",
            "        The weights for each observation in X. If None, all observations\n",
            "        are assigned equal weight (default: None)\n",
            "    \n",
            "    init : {'k-means++', 'random', or ndarray, or a callable}, optional\n",
            "        Method for initialization, default to 'k-means++':\n",
            "    \n",
            "        'k-means++' : selects initial cluster centers for k-mean\n",
            "        clustering in a smart way to speed up convergence. See section\n",
            "        Notes in k_init for more details.\n",
            "    \n",
            "        'random': choose k observations (rows) at random from data for\n",
            "        the initial centroids.\n",
            "    \n",
            "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
            "        and gives the initial centers.\n",
            "    \n",
            "        If a callable is passed, it should take arguments X, k and\n",
            "        and a random state and return an initialization.\n",
            "    \n",
            "    precompute_distances : {'auto', True, False}\n",
            "        Precompute distances (faster but takes more memory).\n",
            "    \n",
            "        'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
            "        million. This corresponds to about 100MB overhead per job using\n",
            "        double precision.\n",
            "    \n",
            "        True : always precompute distances\n",
            "    \n",
            "        False : never precompute distances\n",
            "    \n",
            "    n_init : int, optional, default: 10\n",
            "        Number of time the k-means algorithm will be run with different\n",
            "        centroid seeds. The final results will be the best output of\n",
            "        n_init consecutive runs in terms of inertia.\n",
            "    \n",
            "    max_iter : int, optional, default 300\n",
            "        Maximum number of iterations of the k-means algorithm to run.\n",
            "    \n",
            "    verbose : boolean, optional\n",
            "        Verbosity mode.\n",
            "    \n",
            "    tol : float, optional\n",
            "        The relative increment in the results before declaring convergence.\n",
            "    \n",
            "    random_state : int, RandomState instance or None (default)\n",
            "        Determines random number generation for centroid initialization. Use\n",
            "        an int to make the randomness deterministic.\n",
            "        See :term:`Glossary <random_state>`.\n",
            "    \n",
            "    copy_x : bool, optional\n",
            "        When pre-computing distances it is more numerically accurate to center\n",
            "        the data first.  If copy_x is True (default), then the original data is\n",
            "        not modified, ensuring X is C-contiguous.  If False, the original data\n",
            "        is modified, and put back before the function returns, but small\n",
            "        numerical differences may be introduced by subtracting and then adding\n",
            "        the data mean, in this case it will also not ensure that data is\n",
            "        C-contiguous which may cause a significant slowdown.\n",
            "    \n",
            "    n_jobs : int or None, optional (default=None)\n",
            "        The number of jobs to use for the computation. This works by computing\n",
            "        each of the n_init runs in parallel.\n",
            "    \n",
            "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            "        for more details.\n",
            "    \n",
            "    algorithm : \"auto\", \"full\" or \"elkan\", default=\"auto\"\n",
            "        K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
            "        The \"elkan\" variation is more efficient by using the triangle\n",
            "        inequality, but currently doesn't support sparse data. \"auto\" chooses\n",
            "        \"elkan\" for dense data and \"full\" for sparse data.\n",
            "    \n",
            "    return_n_iter : bool, optional\n",
            "        Whether or not to return the number of iterations.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    centroid : float ndarray with shape (k, n_features)\n",
            "        Centroids found at the last iteration of k-means.\n",
            "    \n",
            "    label : integer ndarray with shape (n_samples,)\n",
            "        label[i] is the code or index of the centroid the\n",
            "        i'th observation is closest to.\n",
            "    \n",
            "    inertia : float\n",
            "        The final value of the inertia criterion (sum of squared distances to\n",
            "        the closest centroid for all observations in the training set).\n",
            "    \n",
            "    best_n_iter : int\n",
            "        Number of iterations corresponding to the best results.\n",
            "        Returned only if `return_n_iter` is set to True.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgzyZ_FqBY1z",
        "colab_type": "text"
      },
      "source": [
        "## **importing Train test split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIppDfg4AaUv",
        "colab_type": "code",
        "outputId": "57a52b59-7cf6-4168-8b9d-03c021c4c4de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "help(train_test_split)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function train_test_split in module sklearn.model_selection._split:\n",
            "\n",
            "train_test_split(*arrays, **options)\n",
            "    Split arrays or matrices into random train and test subsets\n",
            "    \n",
            "    Quick utility that wraps input validation and\n",
            "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
            "    into a single call for splitting (and optionally subsampling) data in a\n",
            "    oneliner.\n",
            "    \n",
            "    Read more in the :ref:`User Guide <cross_validation>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    *arrays : sequence of indexables with same length / shape[0]\n",
            "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
            "        matrices or pandas dataframes.\n",
            "    \n",
            "    test_size : float, int or None, optional (default=None)\n",
            "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
            "        of the dataset to include in the test split. If int, represents the\n",
            "        absolute number of test samples. If None, the value is set to the\n",
            "        complement of the train size. If ``train_size`` is also None, it will\n",
            "        be set to 0.25.\n",
            "    \n",
            "    train_size : float, int, or None, (default=None)\n",
            "        If float, should be between 0.0 and 1.0 and represent the\n",
            "        proportion of the dataset to include in the train split. If\n",
            "        int, represents the absolute number of train samples. If None,\n",
            "        the value is automatically set to the complement of the test size.\n",
            "    \n",
            "    random_state : int, RandomState instance or None, optional (default=None)\n",
            "        If int, random_state is the seed used by the random number generator;\n",
            "        If RandomState instance, random_state is the random number generator;\n",
            "        If None, the random number generator is the RandomState instance used\n",
            "        by `np.random`.\n",
            "    \n",
            "    shuffle : boolean, optional (default=True)\n",
            "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
            "        then stratify must be None.\n",
            "    \n",
            "    stratify : array-like or None (default=None)\n",
            "        If not None, data is split in a stratified fashion, using this as\n",
            "        the class labels.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    splitting : list, length=2 * len(arrays)\n",
            "        List containing train-test split of inputs.\n",
            "    \n",
            "        .. versionadded:: 0.16\n",
            "            If the input is sparse, the output will be a\n",
            "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
            "            input type.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> import numpy as np\n",
            "    >>> from sklearn.model_selection import train_test_split\n",
            "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
            "    >>> X\n",
            "    array([[0, 1],\n",
            "           [2, 3],\n",
            "           [4, 5],\n",
            "           [6, 7],\n",
            "           [8, 9]])\n",
            "    >>> list(y)\n",
            "    [0, 1, 2, 3, 4]\n",
            "    \n",
            "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
            "    ...     X, y, test_size=0.33, random_state=42)\n",
            "    ...\n",
            "    >>> X_train\n",
            "    array([[4, 5],\n",
            "           [0, 1],\n",
            "           [6, 7]])\n",
            "    >>> y_train\n",
            "    [2, 0, 3]\n",
            "    >>> X_test\n",
            "    array([[2, 3],\n",
            "           [8, 9]])\n",
            "    >>> y_test\n",
            "    [1, 4]\n",
            "    \n",
            "    >>> train_test_split(y, shuffle=False)\n",
            "    [[0, 1, 2], [3, 4]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkF6U5BnAuep",
        "colab_type": "code",
        "outputId": "58455547-bdda-4cc3-88fe-f965b781e218",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "help(classification_report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function classification_report in module sklearn.metrics._classification:\n",
            "\n",
            "classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
            "    Build a text report showing the main classification metrics\n",
            "    \n",
            "    Read more in the :ref:`User Guide <classification_report>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
            "        Ground truth (correct) target values.\n",
            "    \n",
            "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
            "        Estimated targets as returned by a classifier.\n",
            "    \n",
            "    labels : array, shape = [n_labels]\n",
            "        Optional list of label indices to include in the report.\n",
            "    \n",
            "    target_names : list of strings\n",
            "        Optional display names matching the labels (same order).\n",
            "    \n",
            "    sample_weight : array-like of shape (n_samples,), default=None\n",
            "        Sample weights.\n",
            "    \n",
            "    digits : int\n",
            "        Number of digits for formatting output floating point values.\n",
            "        When ``output_dict`` is ``True``, this will be ignored and the\n",
            "        returned values will not be rounded.\n",
            "    \n",
            "    output_dict : bool (default = False)\n",
            "        If True, return output as dict\n",
            "    \n",
            "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
            "        Sets the value to return when there is a zero division. If set to\n",
            "        \"warn\", this acts as 0, but warnings are also raised.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    report : string / dict\n",
            "        Text summary of the precision, recall, F1 score for each class.\n",
            "        Dictionary returned if output_dict is True. Dictionary has the\n",
            "        following structure::\n",
            "    \n",
            "            {'label 1': {'precision':0.5,\n",
            "                         'recall':1.0,\n",
            "                         'f1-score':0.67,\n",
            "                         'support':1},\n",
            "             'label 2': { ... },\n",
            "              ...\n",
            "            }\n",
            "    \n",
            "        The reported averages include macro average (averaging the unweighted\n",
            "        mean per label), weighted average (averaging the support-weighted mean\n",
            "        per label), and sample average (only for multilabel classification).\n",
            "        Micro average (averaging the total true positives, false negatives and\n",
            "        false positives) is only shown for multi-label or multi-class\n",
            "        with a subset of classes, because it corresponds to accuracy otherwise.\n",
            "        See also :func:`precision_recall_fscore_support` for more details\n",
            "        on averages.\n",
            "    \n",
            "        Note that in binary classification, recall of the positive class\n",
            "        is also known as \"sensitivity\"; recall of the negative class is\n",
            "        \"specificity\".\n",
            "    \n",
            "    See also\n",
            "    --------\n",
            "    precision_recall_fscore_support, confusion_matrix,\n",
            "    multilabel_confusion_matrix\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.metrics import classification_report\n",
            "    >>> y_true = [0, 1, 2, 2, 2]\n",
            "    >>> y_pred = [0, 0, 2, 2, 1]\n",
            "    >>> target_names = ['class 0', 'class 1', 'class 2']\n",
            "    >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
            "                  precision    recall  f1-score   support\n",
            "    <BLANKLINE>\n",
            "         class 0       0.50      1.00      0.67         1\n",
            "         class 1       0.00      0.00      0.00         1\n",
            "         class 2       1.00      0.67      0.80         3\n",
            "    <BLANKLINE>\n",
            "        accuracy                           0.60         5\n",
            "       macro avg       0.50      0.56      0.49         5\n",
            "    weighted avg       0.70      0.60      0.61         5\n",
            "    <BLANKLINE>\n",
            "    >>> y_pred = [1, 1, 0]\n",
            "    >>> y_true = [1, 1, 1]\n",
            "    >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
            "                  precision    recall  f1-score   support\n",
            "    <BLANKLINE>\n",
            "               1       1.00      0.67      0.80         3\n",
            "               2       0.00      0.00      0.00         0\n",
            "               3       0.00      0.00      0.00         0\n",
            "    <BLANKLINE>\n",
            "       micro avg       1.00      0.67      0.80         3\n",
            "       macro avg       0.33      0.22      0.27         3\n",
            "    weighted avg       1.00      0.67      0.80         3\n",
            "    <BLANKLINE>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjtVabwsBBZt",
        "colab_type": "code",
        "outputId": "071bd36d-2f89-4fd9-c99f-ef4251580175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "help(accuracy_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function accuracy_score in module sklearn.metrics._classification:\n",
            "\n",
            "accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n",
            "    Accuracy classification score.\n",
            "    \n",
            "    In multilabel classification, this function computes subset accuracy:\n",
            "    the set of labels predicted for a sample must *exactly* match the\n",
            "    corresponding set of labels in y_true.\n",
            "    \n",
            "    Read more in the :ref:`User Guide <accuracy_score>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
            "        Ground truth (correct) labels.\n",
            "    \n",
            "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
            "        Predicted labels, as returned by a classifier.\n",
            "    \n",
            "    normalize : bool, optional (default=True)\n",
            "        If ``False``, return the number of correctly classified samples.\n",
            "        Otherwise, return the fraction of correctly classified samples.\n",
            "    \n",
            "    sample_weight : array-like of shape (n_samples,), default=None\n",
            "        Sample weights.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    score : float\n",
            "        If ``normalize == True``, return the fraction of correctly\n",
            "        classified samples (float), else returns the number of correctly\n",
            "        classified samples (int).\n",
            "    \n",
            "        The best performance is 1 with ``normalize == True`` and the number\n",
            "        of samples with ``normalize == False``.\n",
            "    \n",
            "    See also\n",
            "    --------\n",
            "    jaccard_score, hamming_loss, zero_one_loss\n",
            "    \n",
            "    Notes\n",
            "    -----\n",
            "    In binary and multiclass classification, this function is equal\n",
            "    to the ``jaccard_score`` function.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> from sklearn.metrics import accuracy_score\n",
            "    >>> y_pred = [0, 2, 1, 3]\n",
            "    >>> y_true = [0, 1, 2, 3]\n",
            "    >>> accuracy_score(y_true, y_pred)\n",
            "    0.5\n",
            "    >>> accuracy_score(y_true, y_pred, normalize=False)\n",
            "    2\n",
            "    \n",
            "    In the multilabel case with binary label indicators:\n",
            "    \n",
            "    >>> import numpy as np\n",
            "    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
            "    0.5\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "albj6ts_BSKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}